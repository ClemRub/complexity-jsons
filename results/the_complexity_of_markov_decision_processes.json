{
    "title": "The complexity of Markov Decision Processes",
    "authors": [
        "Christos H. Papadimitriou",
        "John N. Tsitsiklis"
    ],
    "results": [
        {
            "mdpType": "MDP",
            "problemType": "Cost Minimization",
            "problemApproach": "Policy Computation",
            "problemNotes": "\"We investigate the complexity of the classical problem of optimal policy computation in Markov decision processes.\"",
            "complexity": "P",
            "complexitysuffix": "complete",
            "horizonType": "Infinite Horizon Average",
            "generalProofType": "Reduction",
            "proofNotes": "3.Theorem 1: Reduction from CVP"
        },
        {
            "mdpType": "MDP",
            "problemType": "Cost Minimization",
            "problemApproach": "Policy Computation",
            "problemNotes": "\"We investigate the complexity of the classical problem of optimal policy computation in Markov decision processes.\"",
            "complexity": "P",
            "complexitysuffix": "complete",
            "horizonType": "Infinite Horizon Discounted",
            "generalProofType": "Reduction",
            "proofNotes": "3.Theorem 1: Reduction from CVP"
        },
        {
            "mdpType": "MDP",
            "problemType": "Cost Minimization",
            "problemApproach": "Policy Computation",
            "problemNotes": "\"We investigate the complexity of the classical problem of optimal policy computation in Markov decision processes.\"",
            "complexity": "P",
            "complexitysuffix": "complete",
            "horizonType": "Finite Horizon",
            "generalProofType": "Reduction",
            "proofNotes": "3.Theorem 1: Reduction from CVP"
        },
        {
            "mdpType": "MDP",
            "problemType": "Cost Minimization",
            "problemApproach": "Policy Computation",
            "problemNotes": "\"We investigate the complexity of the classical problem of optimal policy computation in Markov decision processes.\"",
            "complexity": "NC",
            "horizonType": "Finite Horizon",
            "determinism": "Deterministic",
            "dependence": "Stationary",
            "generalProofType": "Structure",
            "proofNotes": "3.Theorem 2\"Our approach is to look at these problems as variants of the graph-theoretic shortest-path problem[La]"
        },
        {
            "mdpType": "MDP",
            "problemType": "Cost Minimization",
            "problemApproach": "Policy Computation",
            "problemNotes": "\"We investigate the complexity of the classical problem of optimal policy computation in Markov decision processes.\"",
            "complexity": "NC",
            "horizonType": "Finite Horizon",
            "determinism": "Deterministic",
            "dependence": "Nonstationary",
            "generalProofType": "Parallelization",
            "proofNotes": "3. Theorem 2: \"The parallel algorithms that we describe in this Section employ a technique used in the past to yield fast parallel (or space-efficient) algorithms known as \"path doubling\" [Sa, SV]. The idea is, once we have computed all optimal policies between any two states, where each policy starts at time t1 and ends at time t2 , and similarly between t2 and t3, to compute in one step all optimal policies between tl and t3.\""
        },
        {
            "mdpType": "MDP",
            "problemType": "Cost Minimization",
            "problemApproach": "Policy Computation",
            "problemNotes": "\"We investigate the complexity of the classical problem of optimal policy computation in Markov decision processes.\"",
            "complexity": "NC",
            "horizonType": "Infinite Horizon Average",
            "determinism": "Deterministic",
            "generalProofType": "Structure",
            "proofNotes": "3. Theorem 3 \"It is easy to see that the infinite horizon average cost problem is equivalent to finding the cycle in the graph corresponding to the process that is reachable from u_0, and has the smallest average length of arcs. In proof of the equivalence, the limit of the average cost of an infinite path which starts at u_0, reaches this cycle, and then follows this cycle for ever, equals to the optimum average cost, and this cannot be improved.\""
        },
        {
            "mdpType": "MDP",
            "problemType": "Cost Minimization",
            "problemApproach": "Policy Computation",
            "problemNotes": "\"We investigate the complexity of the classical problem of optimal policy computation in Markov decision processes.\"",
            "complexity": "NC",
            "horizonType": "Infinite Horizon Discounted",
            "determinism": "Deterministic",
            "generalProofType": "Structure",
            "proofNotes": "3. Theorem 4. \"Define a sigma in a directed graph to be a path of the form (uo,...,uk, v1,...,vl,v1), where all nodes indicated are distinct. In other words, a sigma is a path from u0 until the first repetition of a node. [...] We can compute the optimum sigma as follows: First, we compute the shortest discounted path of length j = 0, 1..., n along any pair of nodes by \"multiplying\" the matrices ,A, βA... ,β^(j-1)A, where A is the matrix defined in the paragraph before Theorem 3[...]\""
        },
        {
            "mdpType": "MDP",
            "problemType": "Cost Minimization",
            "problemApproach": "Policy Computation",
            "problemNotes": "\"We investigate the complexity of the classical problem of optimal policy computation in Markov decision processes.\"",
            "complexity": "NC",
            "horizonType": "Finite Horizon",
            "determinism": "Deterministic",
            "dependence": "Stationary",
            "generalProofType": "Structure",
            "proofNotes": "\"Given a stationary deterministic process, the stationary finite horizon problem with horizon T is equivalent to finding the shortest path with T arcs in the corresponding graph, starting from the node n_0. The algorithm should run in a number of parallel steps that is polynomial to loglogT; therefore the \"pathdoubling\" technique would not give the desired result. In the sequel we assume thatT>n², otherwise the previous technique applies.\""
        },
        {
            "mdpType": "POMDP",
            "problemType": "Cost Minimization",
            "problemApproach": "Policy Computation",
            "problemNotes": "\"We investigate the complexity of the classical problem of optimal policy computation in Markov decision processes.\"",
            "complexity": "PSPACE",
            "complexitysuffix": "complete",
            "complexityNotes": "\"Even if [Horizon] T is restricted to be smaller than |S|.\" Additionally: \"In the traditional formulation of the partially observed problem [Be], the observation at time t is a random variable, for which we know its conditional distribution given the current state and time. Here we essentially deal with the special case in which the observation is a deterministic function of the current state. Clearly, the general case cannot beany easier.\"",
            "horizonType": "Finite Horizon",
            "dependence": "Stationary",
            "generalProofType": "Reduction",
            "proofNotes": "4. Theorem 6. Reduction from QSAT for hardness. Membership via \"We can think of the possible outcomes of the process within the finite horizon T as a tree of depth T, which has internal nodes both for decisions and transitions. The leaves of this tree can be evaluated to determine the total contribution to the expected cost of this path. To determine whether the optimal policy has expected cost less than some number B, we need to traverse this tree, making nondeterministic decisions at the decision nodes, iterating over all transitions, and taking care to make the same decision every time the history of observations is identical. This latter can be achieved by organizing the search in such a way that nodes at the same level of the tree are divided into intervals from left to right, corresponding to distinct observation sequences. Decisions from nodes in the same-interval must be the same. That the problem is in PSPACE follows now from the fact that PSPACE is robust under nondeterminism.\""
        },
        {
            "mdpType": "POMDP",
            "problemType": "Cost Minimization",
            "problemApproach": "Policy Computation",
            "problemNotes": "\"We investigate the complexity of the classical problem of optimal policy computation in Markov decision processes.\"",
            "complexity": "PSPACE",
            "complexitysuffix": "complete",
            "complexityNotes": "\"Even if [Horizon] T is restricted to be smaller than |S|.\" Additionally: \"In the traditional formulation of the partially observed problem [Be], the observation at time t is a random variable, for which we know its conditional distribution given the current state and time. Here we essentially deal with the special case in which the observation is a deterministic function of the current state. Clearly, the general case cannot beany easier.\"",
            "horizonType": "Finite Horizon",
            "dependence": "Nonstationary",
            "generalProofType": "Reduction",
            "proofNotes": "4. Theorem 6. Reduction from QSAT for hardness. Membership via \"We can think of the possible outcomes of the process within the finite horizon T as a tree of depth T, which has internal nodes both for decisions and transitions. The leaves of this tree can be evaluated to determine the total contribution to the expected cost of this path. To determine whether the optimal policy has expected cost less than some number B, we need to traverse this tree, making nondeterministic decisions at the decision nodes, iterating over all transitions, and taking care to make the same decision every time the history of observations is identical. This latter can be achieved by organizing the search in such a way that nodes at the same level of the tree are divided into intervals from left to right, corresponding to distinct observation sequences. Decisions from nodes in the same-interval must be the same. That the problem is in PSPACE follows now from the fact that PSPACE is robust under nondeterminism.\""
        },
        {
            "mdpType": "Observationless MDP",
            "problemType": "Cost Minimization",
            "problemApproach": "Policy Computation",
            "problemNotes": "\"We investigate the complexity of the classical problem of optimal policy computation in Markov decision processes.\"",
            "complexity": "NP",
            "complexitysuffix": "complete",
            "complexityNotes": "\"Deciding whether the optimal policy in an unobserved Markov decision process has expected cost equal to zero is an NP-complete problem.\"",
            "horizonType": "Not Clearly Stated",
            "determinism": "Nondeterministic",
            "generalProofType": "Consequence of other proof",
            "proofNotes": "4. Corollary 2. \"Finally, considering the special case of the partially observed problem [...] we note that a simplification of the proof of Theorem 6 establishes that this problem is NP-complete\""
        }
    ],
    "url": "https://dspace.mit.edu/bitstream/handle/1721.1/2893/P-1479-13685602.pdf?sequence=1",
    "jsonfilever": "0.0"
}